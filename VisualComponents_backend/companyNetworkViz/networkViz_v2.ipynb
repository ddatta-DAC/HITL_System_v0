{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import networkx as nx\n",
    "import os\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from IPython.display import HTML\n",
    "from joblib import Parallel,delayed\n",
    "import multiprocessing as MP\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('./../')\n",
    "sys.path.append('./../..')\n",
    "from DB_Ingestion.sqlite_engine import sqlite\n",
    "from redisStore import redisUtil\n",
    "import pickle\n",
    "\n",
    "SQL_conn = sqlite().get_engine()\n",
    "DATA_LOC = None\n",
    "subDIR = None\n",
    "html_saveDir = None\n",
    "json_saveDir = None\n",
    "redis_obj = redisUtil.redisStore\n",
    "\n",
    "column_values2id = None\n",
    "DATA_LOC = None\n",
    "subDIR = None\n",
    "html_cache = None\n",
    "DATA_LOC = None\n",
    "saved_emb_loc = None\n",
    "subDIR = None\n",
    "NN_count = 25\n",
    "df_cache = None\n",
    "    \n",
    "# SQL_conn = create_engine('sqlite:///./../../DB/wwf.db', echo=False)\n",
    "\n",
    "# =========================================================================================================\n",
    "def preprocess_data (\n",
    "    num_NN = 10\n",
    "):\n",
    "    global DATA_LOC, saved_emb_loc, subDIR, df_cache\n",
    "    consignee_embedding = np.load(os.path.join(saved_emb_loc,subDIR,'{}_gnn_{}.npy'.format('ConsigneePanjivaID',64))).astype(np.float32)\n",
    "    shipper_embedding = np.load(os.path.join(saved_emb_loc,subDIR, '{}_gnn_{}.npy'.format('ShipperPanjivaID',64))).astype(np.float32)\n",
    "\n",
    "    vec_shipper = shipper_embedding\n",
    "    vec_consignee = consignee_embedding\n",
    "    \n",
    "    tags_consignee = ['Consignee_{}'.format(_) for _ in np.arange(consignee_embedding.shape[0]).astype(int)]\n",
    "    tags_shipper = ['Shipper_{}'.format(_) for _ in np.arange(shipper_embedding.shape[0]).astype(int)]\n",
    "\n",
    "    # Create a local entity to serial id\n",
    "    serial_ids = np.arange(consignee_embedding.shape[0] + shipper_embedding.shape[0] ).astype(int)\n",
    "    entity_id_list = np.arange(consignee_embedding.shape[0]).astype(int).tolist()  + np.arange(shipper_embedding.shape[0]).astype(int).tolist()\n",
    "    serialID2entityID = { k:v for k,v in zip(serial_ids , entity_id_list)}\n",
    "    entityID2serialID = { v:k for k,v in zip(serial_ids , entity_id_list)}\n",
    "    \n",
    "    consignee_serial_ids = np.arange(consignee_embedding.shape[0]).astype(int)\n",
    "    shipper_serial_ids = np.arange(shipper_embedding.shape[0]).astype(int)\n",
    "    \n",
    "    # ===================\n",
    "    # Create FAISS object\n",
    "    # ===================\n",
    "    vectors  = np.vstack([consignee_embedding, shipper_embedding]).astype(np.float32)\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    index.add(vectors.astype(np.float32))\n",
    "\n",
    "    def calc_sim(row):\n",
    "        t = row['ConsigneePanjivaID']\n",
    "        s = row['ShipperPanjivaID']\n",
    "        _sim = 1 - cosine(vec_shipper[s], vec_consignee[t])\n",
    "        return _sim\n",
    "    \n",
    "    # ----------------------\n",
    "    # 1. Read in main data\n",
    "    # 2. Create edges\n",
    "    # 3. Use k-NN to add in supplementary edges ( TODO)\n",
    "    # ----------------------\n",
    "    \n",
    "    train_df = pd.read_csv( os.path.join(DATA_LOC, subDIR, 'train_data.csv'),index_col=None)\n",
    "    df = train_df[['ConsigneePanjivaID','ShipperPanjivaID']].drop_duplicates()\n",
    "    \n",
    "    # Find the nearest neighbor for all consignee\n",
    "    pairs_1 = find_NearestNbrs (\n",
    "        index_obj = index, \n",
    "        serialID2entityID = serialID2entityID,\n",
    "        vector = consignee_embedding, \n",
    "        id_list1 = consignee_serial_ids, \n",
    "        id_list2 = shipper_serial_ids,\n",
    "        num_NN = num_NN\n",
    "    )\n",
    "    \n",
    "    # Find the nearest neighbor for all shippers\n",
    "    pairs_2 = find_NearestNbrs (\n",
    "        index_obj = index,\n",
    "        serialID2entityID = serialID2entityID,\n",
    "        vector = shipper_embedding, \n",
    "        id_list1 = shipper_serial_ids,\n",
    "        id_list2 = consignee_serial_ids,\n",
    "        num_NN = num_NN\n",
    "    )\n",
    "\n",
    "    pairs_1 = np.array(pairs_1)\n",
    "    pairs_2 = np.array(pairs_2)\n",
    "    \n",
    "    tmpdf_1 = pd.DataFrame( {\n",
    "        'ConsigneePanjivaID': pairs_1[:,0],\n",
    "        'ShipperPanjivaID': pairs_1[:,1]\n",
    "    })\n",
    "    \n",
    "    tmpdf_2 = pd.DataFrame( {\n",
    "        'ShipperPanjivaID': pairs_2[:,0],\n",
    "        'ConsigneePanjivaID': pairs_2[:,1]\n",
    "    })\n",
    "    \n",
    "    df_1 = tmpdf_1.copy()\n",
    "    df_1 = df_1.append(tmpdf_2, ignore_index=True)\n",
    "    df_1 = df_1.drop_duplicates()\n",
    "    \n",
    "    df_1['weight'] = df_1.apply(calc_sim, axis=1).reset_index(drop=True)\n",
    "    # Normalize the wights\n",
    "    _min = np.min(df_1['weight'])\n",
    "    _max = np.max(df_1['weight'])\n",
    "    df_1['weight'] =  df_1['weight'].apply(lambda x: (x-_min)/(_max-_min))\n",
    "    df_projected =  df_1.copy(deep=True)\n",
    "    \n",
    "    # ------------------------------------\n",
    "    # Add in the actual links\n",
    "    # ------------------------------------\n",
    "    df_actual = train_df.groupby(['ConsigneePanjivaID','ShipperPanjivaID']).size().reset_index(name='weight')\n",
    "    df_actual['ConsigneePanjivaID'] = df_actual['ConsigneePanjivaID'].apply(lambda x: 'ConsigneePanjivaID-{}'.format(x))\n",
    "    df_actual['ShipperPanjivaID'] = df_actual['ShipperPanjivaID'].apply(lambda x: 'ShipperPanjivaID-{}'.format(x)) \n",
    "    df_projected['ConsigneePanjivaID'] = df_projected['ConsigneePanjivaID'].apply(lambda x: 'ConsigneePanjivaID-{}'.format(x))\n",
    "    df_projected['ShipperPanjivaID'] = df_projected['ShipperPanjivaID'].apply(lambda x: 'ShipperPanjivaID-{}'.format(x)) \n",
    "   \n",
    "    fname = os.path.join(df_cache,'df_actual.csv')\n",
    "    df_actual.to_csv(fname, index=None)\n",
    "    fname = os.path.join(df_cache,'df_projected.csv')\n",
    "    df_projected.to_csv(fname, index=None)\n",
    "    return  \n",
    "\n",
    "# ============================================\n",
    "# Find the nearest neigbhbors using Embedding\n",
    "# The return is a list of pair of entity_ids\n",
    "# ============================================\n",
    "def find_NearestNbrs(\n",
    "    index_obj, \n",
    "    serialID2entityID, \n",
    "    vector,\n",
    "    id_list1, \n",
    "    id_list2, \n",
    "    num_NN\n",
    "): \n",
    "    print('Finding nearest neighbors...')\n",
    "    distances, NN_ids = index_obj.search( vector, k = num_NN*4)  \n",
    "    # ---------------------------------------------------\n",
    "    # Filter nbr to be of other type (bipartite graph)\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    # vectors are ordered internally (intra-domain)\n",
    "    entity_ids = [ serialID2entityID[i] for i in id_list1 ]\n",
    "    \n",
    "    def aux_check(_id_, nn_list, validation_list, num_NN):\n",
    "        filtered_nn = [ _nbr for _nbr in nn_list if _nbr in validation_list][:num_NN]\n",
    "        return ( _id_, filtered_nn)\n",
    "    \n",
    "    validation_list = id_list2\n",
    "    res = Parallel(n_jobs=MP.cpu_count()) (\n",
    "        delayed(aux_check)( _id_ , _nn_, validation_list, num_NN ) for _id_, _nn_ in zip(entity_ids , NN_ids)\n",
    "    )\n",
    "    \n",
    "    pairs = []\n",
    "    for pair in res:\n",
    "        for _item in pair[1]:\n",
    "            pairs.append((pair[0], serialID2entityID[_item]))\n",
    "    print(len(pairs))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize(\n",
    "    _DATA_LOC,\n",
    "    _subDIR,\n",
    "    _saved_emb_loc,\n",
    "    _df_cache,\n",
    "    _html_cache\n",
    "):\n",
    "    global DATA_LOC, subDIR, redis_obj, column_values2id, NN_count, df_cache, html_cache, saved_emb_loc\n",
    "    DATA_LOC = _DATA_LOC\n",
    "    subDIR = _subDIR\n",
    "    saved_emb_loc = _saved_emb_loc\n",
    "   \n",
    "    with open(os.path.join(DATA_LOC, subDIR,'col_val2id_dict.pkl'), 'rb') as fh:\n",
    "        column_values2id = pickle.load(fh)\n",
    "    df_cache  = _df_cache\n",
    "    html_cache = _html_cache\n",
    "    Path(df_cache).mkdir(exist_ok=True,parents=True)\n",
    "    Path(html_cache).mkdir(exist_ok=True,parents=True)\n",
    "    redis_obj.ingest_record_data(DATA_LOC, subDIR)\n",
    "    preprocess_data(10)\n",
    "    return \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def obtain_node_display_data(node_type, _id):\n",
    "    global SQL_conn\n",
    "    global DATA_LOC, subDIR\n",
    "    global column_values2id\n",
    "    \n",
    "    # get the actual value from the id passed\n",
    "    __ID__ = [value  for value, e_id  in column_values2id[node_type].items() if e_id == int(_id)][0]\n",
    "    display_str = ''\n",
    "    \n",
    "    if node_type == 'ConsigneePanjivaID':\n",
    "        df = pd.read_sql(\n",
    "                \"select {},{},{},{} from {} where {}={}\".format(\n",
    "                    'ConsigneePanjivaID', 'ConsigneeName', 'ConsigneeCity', 'ConsigneeCountry', 'ConsigneePanjivaID', 'ConsigneePanjivaID', __ID__\n",
    "                ), \n",
    "                SQL_conn,\n",
    "                index_col=None\n",
    "        )\n",
    "        display_str = 'Consignee :: '\n",
    "        display_str += '<br>' .join(\n",
    "                [str(df['ConsigneeName'].values[0]),str(df['ConsigneeCity'].values[0]),str(df['ConsigneeCountry'].values[0])]\n",
    "        )\n",
    "        \n",
    "        df = pd.read_sql('select count(*) as count  from Records where ConsigneePanjivaID={}'.format(__ID__), SQL_conn, index_col=None)\n",
    "        count = df['count'].values[0]\n",
    "        \n",
    "    if node_type == 'ShipperPanjivaID':\n",
    "        df = pd.read_sql(\n",
    "                \"select {},{},{},{} from {} where {}={}\".format(\n",
    "                    'ShipperPanjivaID', 'ShipperName', 'ShipperCity', 'ShipperCountry', 'ShipperPanjivaID', 'ShipperPanjivaID', __ID__\n",
    "                ), \n",
    "                SQL_conn,\n",
    "                index_col=None\n",
    "        )\n",
    "        display_str = 'Shipper :: '\n",
    "        display_str += '<br>' .join(\n",
    "            [str(df['ShipperName'].values[0]), str(df['ShipperCity'].values[0]), str(df['ShipperCountry'].values[0])]\n",
    "        )\n",
    "        df = pd.read_sql('select count(*) as count from Records where ShipperPanjivaID={}'.format(__ID__),  SQL_conn, index_col=None)\n",
    "        count = df['count'].values[0]\n",
    "    count = int (10 + (np.log10(count)+1)*2)\n",
    "    return __ID__, display_str,count\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Main function\n",
    "# ================================================================\n",
    "def visualize(\n",
    "    PanjivaRecordID,\n",
    "    fig_height = '720px',\n",
    "    fig_width = '100%',\n",
    "    title=False,\n",
    "    return_type=2\n",
    "):\n",
    "    global DATA_LOC, subDIR, redis_obj, df_cache, html_cache\n",
    "    signature_fname = 'consigneeShipper_{}_{}_{}.html'.format(PanjivaRecordID,fig_height,fig_width) \n",
    "    \n",
    "    f_path = os.path.join(\n",
    "        html_cache, \n",
    "        signature_fname\n",
    "    )\n",
    "    if os.path.exists(os.path.join(html_cache, signature_fname)) and return_type==2:\n",
    "        return os.path.join(html_cache, signature_fname)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Read in saved dataframes \n",
    "    df_actual = pd.read_csv(os.path.join(df_cache, 'df_actual.csv'),index_col=None)\n",
    "    \n",
    "    df_0 = pd.read_csv(os.path.join(df_cache, 'df_actual.csv'),index_col=None)\n",
    "    df_1 = pd.read_csv(os.path.join(df_cache, 'df_projected.csv'),index_col=None)\n",
    "    \n",
    "    # -----------------------\n",
    "    record = redis_obj.fetch_data(str(PanjivaRecordID))\n",
    "    print('record >>', record)\n",
    "    \n",
    "    consignee = 'ConsigneePanjivaID-' + str(record['ConsigneePanjivaID'])\n",
    "    shipper = 'ShipperPanjivaID-' + str(record['ShipperPanjivaID'])\n",
    "    \n",
    "    if consignee is not None and  shipper is not None:\n",
    "        df_0 = df_0.loc[(df_0['ConsigneePanjivaID']==consignee) | (df_0['ShipperPanjivaID']==shipper)]\n",
    "        \n",
    "\n",
    "    df_pred_consignee = df_1.loc[ (df_1['ShipperPanjivaID']==shipper)].sort_values(by='weight',ascending=False).head(10)\n",
    "    df_pred_shipper = df_1.loc[(df_1['ConsigneePanjivaID']==consignee) ].sort_values(by='weight',ascending=False).head(10)\n",
    "    \n",
    "    if title is True:\n",
    "        title_text = 'Network of Consignee & Shippers'\n",
    "    else:\n",
    "        title_text = ''\n",
    "    # ---------------------------\n",
    "\n",
    "    # Add in secondary edges\n",
    "    df_2 = df_actual.loc[\n",
    "        (df_actual['ConsigneePanjivaID'].isin(df_pred_consignee['ConsigneePanjivaID'])) |\n",
    "        (df_actual['ShipperPanjivaID'].isin(df_pred_shipper['ShipperPanjivaID']))\n",
    "    ]\n",
    "    # ---------------------------\n",
    "    # Create a networkx graph \n",
    "    nx_graph = nx.Graph()\n",
    "    # Add nodes\n",
    "    consignee_nodes = df_0['ConsigneePanjivaID'].values.tolist() \n",
    "    consignee_nodes+= df_pred_consignee['ConsigneePanjivaID'].values.tolist()  \n",
    "    consignee_nodes+= df_pred_shipper['ConsigneePanjivaID'].values.tolist() \n",
    "    consignee_nodes+= df_2['ConsigneePanjivaID'].values.tolist()\n",
    "    consignee_nodes = list(set(consignee_nodes))\n",
    "    \n",
    "    shipper_nodes = df_0['ShipperPanjivaID'].values.tolist() \n",
    "    shipper_nodes += df_pred_consignee['ShipperPanjivaID'].values.tolist() \n",
    "    shipper_nodes += df_pred_shipper['ShipperPanjivaID'].values.tolist()\n",
    "    shipper_nodes += df_2['ShipperPanjivaID'].values.tolist()\n",
    "    shipper_nodes = list(set(shipper_nodes))\n",
    "   \n",
    "    \n",
    "    # Obtain node data \n",
    "    def aux_get_data(node_id):\n",
    "        node_type, _id = node_id.split('-')\n",
    "        _ID_, node_descriptor,node_size = obtain_node_display_data(node_type, _id)\n",
    "        return (node_id, node_descriptor, _ID_, node_size )\n",
    "        \n",
    "    node_data_consignee = Parallel(n_jobs=MP.cpu_count(),prefer=\"threads\")(\n",
    "        delayed(aux_get_data)(node,) for node in consignee_nodes\n",
    "    )\n",
    "    node_data_shipper = Parallel(n_jobs=MP.cpu_count(),prefer=\"threads\")(\n",
    "        delayed(aux_get_data)(node,) for node in shipper_nodes\n",
    "    )\n",
    "    for cn in node_data_consignee:\n",
    "        nx_graph.add_node( cn[0], label = cn[2], title = cn[1], size = cn[3], color='Coral')\n",
    "    \n",
    "    for cn in node_data_shipper:\n",
    "        nx_graph.add_node( cn[0], label = cn[2], title = cn[1], size = cn[3], color='MediumSlateBlue')\n",
    "        \n",
    "    \n",
    "    for _type, _df in zip(['actual','actual','predicted','predicted'],[df_0, df_2, df_pred_consignee, df_pred_shipper]):\n",
    "        sources = _df['ConsigneePanjivaID']\n",
    "        targets = _df['ShipperPanjivaID']\n",
    "        weights = _df['weight']\n",
    "        edge_data = zip(sources, targets, weights)\n",
    "        for e in edge_data:\n",
    "            src = e[0]\n",
    "            dst = e[1]\n",
    "            w = e[2]\n",
    "            if _type == 'predicted' and (src==shipper or dst == consignee or dst==shipper or src == consignee ):\n",
    "                nx_graph.add_edge(src, dst, weight= w, color='red')\n",
    "            else:\n",
    "                nx_graph.add_edge(src, dst, weight= w, color='blue')\n",
    "    \n",
    "    largest_cc = max(nx.connected_components(nx_graph), key=len)\n",
    "    G = nx_graph.subgraph(largest_cc).copy()\n",
    "    \n",
    "    net = Network(\n",
    "        height=fig_height, \n",
    "        width=fig_width,\n",
    "        bgcolor=\"white\", \n",
    "        font_color=\"black\", \n",
    "        notebook=False,\n",
    "        heading= title_text\n",
    "    )\n",
    "    \n",
    "    net.from_nx(G)\n",
    "    net.barnes_hut() \n",
    "    net.set_options(\n",
    "    \"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidthSelected\": 4,\n",
    "        \"color\": {\n",
    "          \"background\": \"rgba(163,252,11,1)\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    f_path = os.path.join(html_cache, signature_fname)\n",
    "    net.write_html(f_path)\n",
    "    if return_type == 1:\n",
    "        return net\n",
    "    else:\n",
    "        return f_path\n",
    "    \n",
    "# ==============================================================================\n",
    "# initialize(\n",
    "#     _DATA_LOC ='./../../generated_data_v1/us_import',\n",
    "#     _saved_emb_loc =  './../../GNN/saved_model_gnn',\n",
    "#     _subDIR = '01_2016',\n",
    "#     _html_cache= 'htmlCache',\n",
    "#     _df_cache = 'dfCache'\n",
    "# )\n",
    "\n",
    "# html_path = visualize( \n",
    "#     PanjivaRecordID ='120888026',\n",
    "#     fig_width='100%', \n",
    "#     title=False, \n",
    "#     fig_height='920px', \n",
    "#     return_type = 2\n",
    "# )\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "prompt-martin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "daily-macedonia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "                 \n",
    "\n",
    "\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# net.show_buttons(filter_=['nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "coated-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.show('nx.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
